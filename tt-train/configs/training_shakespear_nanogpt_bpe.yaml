training_config:
  project_name: "tt_train_nano_gpt"
  seed: 5489
  model_save_interval: 500
  batch_size: 8
  num_epochs: 1
  max_steps: 5000
  learning_rate: 0.0002
  weight_decay: 0.1
  tokenizer_type: bpe
  use_moreh_adamw: true
  use_kahan_summation: false
  gradient_accumulation_steps: 8
  scheduler_type: "warmup_linear"
  using_clip_grad_norm: true
  transformer_config:
    weight_tying: enabled
    num_heads: 6
    embedding_dim: 384
    dropout_prob: 0.2
    num_blocks: 6
    vocab_size: 96
    max_sequence_length: 256
    positional_embedding_type: trainable
    experimental:
      use_composite_layernorm: false
