training_config:
  project_name: "tt_train_nano_gpt"
  seed: 5489
  model_save_interval: 100
  model_path: "ttnn_gpt2s_gpt_1024_data.msgpack"
  batch_size: 8
  num_epochs: 1
  max_steps: 5000 #2.5B tokens used for training
  learning_rate: 0.0003
  weight_decay: 0.1
  use_moreh_adamw: true
  use_kahan_summation: false
  gradient_accumulation_steps: 16
  tokenizer_path: "data/train_ttnn/tokenizer.json"
  tokenizer_type: bpe
  data_path: "data/train_ttnn/data.txt"
  scheduler_type: "warmup_linear"
  using_clip_grad_norm: true

  transformer_config:
    weight_tying: enabled
    runner_type: memory_efficient
    num_heads: 12
    embedding_dim: 768
    dropout_prob: 0.1
    num_blocks: 12
    vocab_size: 224
    max_sequence_length: 1024
    experimental:
      use_composite_layernorm: false
eval_config:
  repetition_penalty: 2.0
  temperature: 0.7
  top_k: 100
  top_p: 1.0
