<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url(https://themes.googleusercontent.com/fonts/css?kit=OPeqXG-QxW3ZD8BtmPikfA);ul.lst-kix_7l3izwcooizl-1{list-style-type:none}.lst-kix_sdnmvxmx7mzu-6>li{counter-increment:lst-ctn-kix_sdnmvxmx7mzu-6}ul.lst-kix_7l3izwcooizl-2{list-style-type:none}ul.lst-kix_7l3izwcooizl-3{list-style-type:none}ul.lst-kix_7l3izwcooizl-4{list-style-type:none}ul.lst-kix_7l3izwcooizl-5{list-style-type:none}.lst-kix_fm52h3pm6g24-8>li:before{content:"-  "}ul.lst-kix_7l3izwcooizl-6{list-style-type:none}ul.lst-kix_7l3izwcooizl-7{list-style-type:none}.lst-kix_fm52h3pm6g24-7>li:before{content:"-  "}ul.lst-kix_7l3izwcooizl-8{list-style-type:none}.lst-kix_fm52h3pm6g24-2>li:before{content:"-  "}ol.lst-kix_sdnmvxmx7mzu-5.start{counter-reset:lst-ctn-kix_sdnmvxmx7mzu-5 0}.lst-kix_fm52h3pm6g24-4>li:before{content:"-  "}.lst-kix_fm52h3pm6g24-3>li:before{content:"-  "}.lst-kix_fm52h3pm6g24-6>li:before{content:"-  "}.lst-kix_k11a6ms878zw-0>li:before{content:"-  "}.lst-kix_k11a6ms878zw-1>li:before{content:"-  "}.lst-kix_fm52h3pm6g24-5>li:before{content:"-  "}.lst-kix_panwxdneeeuv-1>li:before{content:"-  "}.lst-kix_panwxdneeeuv-2>li:before{content:"-  "}.lst-kix_panwxdneeeuv-0>li:before{content:"-  "}.lst-kix_panwxdneeeuv-3>li:before{content:"-  "}.lst-kix_panwxdneeeuv-4>li:before{content:"-  "}.lst-kix_panwxdneeeuv-5>li:before{content:"-  "}.lst-kix_panwxdneeeuv-6>li:before{content:"-  "}.lst-kix_sdnmvxmx7mzu-8>li{counter-increment:lst-ctn-kix_sdnmvxmx7mzu-8}ul.lst-kix_ek96qq2xmhch-8{list-style-type:none}ul.lst-kix_ek96qq2xmhch-6{list-style-type:none}ul.lst-kix_2bdje4yk4iw-2{list-style-type:none}ul.lst-kix_ek96qq2xmhch-7{list-style-type:none}ul.lst-kix_2bdje4yk4iw-3{list-style-type:none}ul.lst-kix_ek96qq2xmhch-4{list-style-type:none}ul.lst-kix_x1gdfe6t2wts-1{list-style-type:none}ul.lst-kix_2bdje4yk4iw-4{list-style-type:none}ul.lst-kix_ek96qq2xmhch-5{list-style-type:none}ul.lst-kix_x1gdfe6t2wts-0{list-style-type:none}ul.lst-kix_2bdje4yk4iw-5{list-style-type:none}ul.lst-kix_ek96qq2xmhch-2{list-style-type:none}ul.lst-kix_ek96qq2xmhch-3{list-style-type:none}ul.lst-kix_ek96qq2xmhch-0{list-style-type:none}ul.lst-kix_2bdje4yk4iw-0{list-style-type:none}ul.lst-kix_ek96qq2xmhch-1{list-style-type:none}ul.lst-kix_2bdje4yk4iw-1{list-style-type:none}ul.lst-kix_x1gdfe6t2wts-7{list-style-type:none}ul.lst-kix_x1gdfe6t2wts-6{list-style-type:none}ul.lst-kix_x1gdfe6t2wts-8{list-style-type:none}ul.lst-kix_x1gdfe6t2wts-3{list-style-type:none}ul.lst-kix_2bdje4yk4iw-6{list-style-type:none}ul.lst-kix_x1gdfe6t2wts-2{list-style-type:none}ul.lst-kix_2bdje4yk4iw-7{list-style-type:none}ul.lst-kix_x1gdfe6t2wts-5{list-style-type:none}ul.lst-kix_2bdje4yk4iw-8{list-style-type:none}ul.lst-kix_x1gdfe6t2wts-4{list-style-type:none}.lst-kix_9l64gnuy3fl7-7>li:before{content:"-  "}.lst-kix_9l64gnuy3fl7-8>li:before{content:"-  "}.lst-kix_9l64gnuy3fl7-5>li:before{content:"-  "}.lst-kix_9l64gnuy3fl7-3>li:before{content:"-  "}.lst-kix_9l64gnuy3fl7-4>li:before{content:"-  "}.lst-kix_9l64gnuy3fl7-2>li:before{content:"-  "}ul.lst-kix_nshgg3uvyo9d-0{list-style-type:none}ul.lst-kix_nshgg3uvyo9d-2{list-style-type:none}ul.lst-kix_nshgg3uvyo9d-1{list-style-type:none}ul.lst-kix_nshgg3uvyo9d-4{list-style-type:none}ul.lst-kix_nshgg3uvyo9d-3{list-style-type:none}ul.lst-kix_nshgg3uvyo9d-6{list-style-type:none}ul.lst-kix_nshgg3uvyo9d-5{list-style-type:none}.lst-kix_nshgg3uvyo9d-0>li:before{content:"-  "}ul.lst-kix_nshgg3uvyo9d-8{list-style-type:none}ul.lst-kix_nshgg3uvyo9d-7{list-style-type:none}.lst-kix_9l64gnuy3fl7-6>li:before{content:"-  "}.lst-kix_nshgg3uvyo9d-1>li:before{content:"-  "}.lst-kix_k11a6ms878zw-3>li:before{content:"-  "}.lst-kix_k11a6ms878zw-2>li:before{content:"-  "}.lst-kix_k11a6ms878zw-4>li:before{content:"-  "}.lst-kix_k11a6ms878zw-7>li:before{content:"-  "}.lst-kix_k11a6ms878zw-6>li:before{content:"-  "}.lst-kix_k11a6ms878zw-5>li:before{content:"-  "}.lst-kix_fm52h3pm6g24-1>li:before{content:"-  "}.lst-kix_9l64gnuy3fl7-0>li:before{content:"-  "}.lst-kix_fm52h3pm6g24-0>li:before{content:"-  "}.lst-kix_9l64gnuy3fl7-1>li:before{content:"-  "}.lst-kix_k11a6ms878zw-8>li:before{content:"-  "}.lst-kix_sdnmvxmx7mzu-4>li{counter-increment:lst-ctn-kix_sdnmvxmx7mzu-4}ul.lst-kix_7l3izwcooizl-0{list-style-type:none}ul.lst-kix_9l64gnuy3fl7-3{list-style-type:none}ul.lst-kix_9l64gnuy3fl7-4{list-style-type:none}ul.lst-kix_9l64gnuy3fl7-5{list-style-type:none}ul.lst-kix_9l64gnuy3fl7-6{list-style-type:none}ul.lst-kix_9l64gnuy3fl7-0{list-style-type:none}ul.lst-kix_9l64gnuy3fl7-1{list-style-type:none}ul.lst-kix_9l64gnuy3fl7-2{list-style-type:none}.lst-kix_fwvmi50r8yw-4>li:before{content:"-  "}ul.lst-kix_panwxdneeeuv-0{list-style-type:none}ul.lst-kix_panwxdneeeuv-1{list-style-type:none}ul.lst-kix_panwxdneeeuv-2{list-style-type:none}.lst-kix_sbjhxah0gf4r-5>li:before{content:"-  "}ol.lst-kix_sdnmvxmx7mzu-4.start{counter-reset:lst-ctn-kix_sdnmvxmx7mzu-4 0}.lst-kix_fwvmi50r8yw-6>li:before{content:"-  "}.lst-kix_sbjhxah0gf4r-7>li:before{content:"-  "}.lst-kix_fwvmi50r8yw-8>li:before{content:"-  "}.lst-kix_nshgg3uvyo9d-3>li:before{content:"-  "}.lst-kix_nshgg3uvyo9d-7>li:before{content:"-  "}.lst-kix_2bdje4yk4iw-7>li:before{content:"-  "}.lst-kix_nshgg3uvyo9d-5>li:before{content:"-  "}.lst-kix_2bdje4yk4iw-5>li:before{content:"-  "}.lst-kix_x1gdfe6t2wts-2>li:before{content:"-  "}.lst-kix_x1gdfe6t2wts-4>li:before{content:"-  "}.lst-kix_x1gdfe6t2wts-0>li:before{content:"-  "}.lst-kix_2bdje4yk4iw-3>li:before{content:"-  "}ul.lst-kix_fm52h3pm6g24-0{list-style-type:none}ul.lst-kix_fm52h3pm6g24-1{list-style-type:none}ul.lst-kix_9l64gnuy3fl7-7{list-style-type:none}ul.lst-kix_9l64gnuy3fl7-8{list-style-type:none}.lst-kix_2bdje4yk4iw-1>li:before{content:"-  "}ul.lst-kix_fm52h3pm6g24-8{list-style-type:none}ul.lst-kix_fm52h3pm6g24-6{list-style-type:none}ul.lst-kix_fm52h3pm6g24-7{list-style-type:none}ul.lst-kix_fm52h3pm6g24-4{list-style-type:none}ul.lst-kix_fm52h3pm6g24-5{list-style-type:none}ul.lst-kix_fm52h3pm6g24-2{list-style-type:none}ul.lst-kix_fm52h3pm6g24-3{list-style-type:none}.lst-kix_k7awirwczdtb-1>li:before{content:"-  "}.lst-kix_k7awirwczdtb-3>li:before{content:"-  "}.lst-kix_sdnmvxmx7mzu-3>li{counter-increment:lst-ctn-kix_sdnmvxmx7mzu-3}.lst-kix_k7awirwczdtb-5>li:before{content:"-  "}.lst-kix_panwxdneeeuv-8>li:before{content:"-  "}ol.lst-kix_sdnmvxmx7mzu-0{list-style-type:none}.lst-kix_ok3a0zuyki56-1>li:before{content:"-  "}ol.lst-kix_sdnmvxmx7mzu-2{list-style-type:none}ol.lst-kix_sdnmvxmx7mzu-1{list-style-type:none}ol.lst-kix_sdnmvxmx7mzu-4{list-style-type:none}.lst-kix_sdnmvxmx7mzu-2>li{counter-increment:lst-ctn-kix_sdnmvxmx7mzu-2}ol.lst-kix_sdnmvxmx7mzu-3{list-style-type:none}ol.lst-kix_sdnmvxmx7mzu-6{list-style-type:none}ol.lst-kix_sdnmvxmx7mzu-5{list-style-type:none}ol.lst-kix_sdnmvxmx7mzu-7.start{counter-reset:lst-ctn-kix_sdnmvxmx7mzu-7 0}.lst-kix_k7awirwczdtb-7>li:before{content:"-  "}ol.lst-kix_sdnmvxmx7mzu-8{list-style-type:none}ol.lst-kix_sdnmvxmx7mzu-7{list-style-type:none}.lst-kix_sdnmvxmx7mzu-1>li:before{content:"" counter(lst-ctn-kix_sdnmvxmx7mzu-1,lower-latin) ". "}ul.lst-kix_fwvmi50r8yw-0{list-style-type:none}.lst-kix_ok3a0zuyki56-5>li:before{content:"-  "}ul.lst-kix_fwvmi50r8yw-4{list-style-type:none}ul.lst-kix_fwvmi50r8yw-3{list-style-type:none}ul.lst-kix_fwvmi50r8yw-2{list-style-type:none}ul.lst-kix_fwvmi50r8yw-1{list-style-type:none}ul.lst-kix_fwvmi50r8yw-8{list-style-type:none}ul.lst-kix_fwvmi50r8yw-7{list-style-type:none}ul.lst-kix_fwvmi50r8yw-6{list-style-type:none}.lst-kix_ok3a0zuyki56-3>li:before{content:"-  "}ul.lst-kix_fwvmi50r8yw-5{list-style-type:none}ul.lst-kix_panwxdneeeuv-3{list-style-type:none}.lst-kix_fwvmi50r8yw-2>li:before{content:"-  "}ul.lst-kix_panwxdneeeuv-4{list-style-type:none}.lst-kix_sbjhxah0gf4r-3>li:before{content:"-  "}ul.lst-kix_panwxdneeeuv-5{list-style-type:none}ul.lst-kix_panwxdneeeuv-6{list-style-type:none}ul.lst-kix_panwxdneeeuv-7{list-style-type:none}ul.lst-kix_panwxdneeeuv-8{list-style-type:none}ol.lst-kix_sdnmvxmx7mzu-6.start{counter-reset:lst-ctn-kix_sdnmvxmx7mzu-6 0}.lst-kix_sbjhxah0gf4r-1>li:before{content:"-  "}.lst-kix_fwvmi50r8yw-0>li:before{content:"-  "}.lst-kix_ok3a0zuyki56-7>li:before{content:"-  "}.lst-kix_7l3izwcooizl-6>li:before{content:"-  "}.lst-kix_7l3izwcooizl-8>li:before{content:"-  "}ol.lst-kix_sdnmvxmx7mzu-8.start{counter-reset:lst-ctn-kix_sdnmvxmx7mzu-8 0}.lst-kix_7l3izwcooizl-5>li:before{content:"-  "}.lst-kix_7l3izwcooizl-7>li:before{content:"-  "}.lst-kix_7l3izwcooizl-0>li:before{content:"-  "}.lst-kix_sdnmvxmx7mzu-5>li{counter-increment:lst-ctn-kix_sdnmvxmx7mzu-5}.lst-kix_7l3izwcooizl-1>li:before{content:"-  "}.lst-kix_7l3izwcooizl-2>li:before{content:"-  "}.lst-kix_7l3izwcooizl-4>li:before{content:"-  "}.lst-kix_7l3izwcooizl-3>li:before{content:"-  "}.lst-kix_sdnmvxmx7mzu-2>li:before{content:"" counter(lst-ctn-kix_sdnmvxmx7mzu-2,lower-roman) ". "}.lst-kix_ek96qq2xmhch-7>li:before{content:"-  "}.lst-kix_ek96qq2xmhch-8>li:before{content:"-  "}ol.lst-kix_sdnmvxmx7mzu-2.start{counter-reset:lst-ctn-kix_sdnmvxmx7mzu-2 0}.lst-kix_sdnmvxmx7mzu-3>li:before{content:"" counter(lst-ctn-kix_sdnmvxmx7mzu-3,decimal) ". "}.lst-kix_sdnmvxmx7mzu-4>li:before{content:"" counter(lst-ctn-kix_sdnmvxmx7mzu-4,lower-latin) ". "}ul.lst-kix_k11a6ms878zw-7{list-style-type:none}ul.lst-kix_k11a6ms878zw-8{list-style-type:none}ul.lst-kix_k11a6ms878zw-5{list-style-type:none}ul.lst-kix_k11a6ms878zw-6{list-style-type:none}.lst-kix_sdnmvxmx7mzu-5>li:before{content:"" counter(lst-ctn-kix_sdnmvxmx7mzu-5,lower-roman) ". "}.lst-kix_sdnmvxmx7mzu-7>li:before{content:"" counter(lst-ctn-kix_sdnmvxmx7mzu-7,lower-latin) ". "}ul.lst-kix_k11a6ms878zw-3{list-style-type:none}ul.lst-kix_k11a6ms878zw-4{list-style-type:none}ul.lst-kix_k11a6ms878zw-1{list-style-type:none}ul.lst-kix_k11a6ms878zw-2{list-style-type:none}.lst-kix_sdnmvxmx7mzu-6>li:before{content:"" counter(lst-ctn-kix_sdnmvxmx7mzu-6,decimal) ". "}ul.lst-kix_k11a6ms878zw-0{list-style-type:none}.lst-kix_sdnmvxmx7mzu-8>li:before{content:"" counter(lst-ctn-kix_sdnmvxmx7mzu-8,lower-roman) ". "}.lst-kix_x1gdfe6t2wts-8>li:before{content:"-  "}.lst-kix_x1gdfe6t2wts-6>li:before{content:"-  "}.lst-kix_x1gdfe6t2wts-7>li:before{content:"-  "}.lst-kix_x1gdfe6t2wts-5>li:before{content:"-  "}.lst-kix_ek96qq2xmhch-6>li:before{content:"-  "}.lst-kix_wdqip9honobb-0>li:before{content:"\0025cf   "}.lst-kix_wdqip9honobb-2>li:before{content:"\0025a0   "}.lst-kix_ek96qq2xmhch-5>li:before{content:"-  "}.lst-kix_wdqip9honobb-3>li:before{content:"\0025cf   "}.lst-kix_ek96qq2xmhch-3>li:before{content:"-  "}.lst-kix_ek96qq2xmhch-2>li:before{content:"-  "}.lst-kix_ek96qq2xmhch-4>li:before{content:"-  "}.lst-kix_wdqip9honobb-1>li:before{content:"\0025cb   "}ul.lst-kix_ok3a0zuyki56-8{list-style-type:none}.lst-kix_ek96qq2xmhch-0>li:before{content:"-  "}ul.lst-kix_ok3a0zuyki56-6{list-style-type:none}.lst-kix_wdqip9honobb-8>li:before{content:"\0025a0   "}ul.lst-kix_ok3a0zuyki56-7{list-style-type:none}.lst-kix_ek96qq2xmhch-1>li:before{content:"-  "}ul.lst-kix_ok3a0zuyki56-4{list-style-type:none}ul.lst-kix_ok3a0zuyki56-5{list-style-type:none}ul.lst-kix_ok3a0zuyki56-2{list-style-type:none}.lst-kix_wdqip9honobb-7>li:before{content:"\0025cb   "}.lst-kix_sdnmvxmx7mzu-1>li{counter-increment:lst-ctn-kix_sdnmvxmx7mzu-1}ul.lst-kix_ok3a0zuyki56-3{list-style-type:none}ol.lst-kix_sdnmvxmx7mzu-3.start{counter-reset:lst-ctn-kix_sdnmvxmx7mzu-3 0}ul.lst-kix_ok3a0zuyki56-0{list-style-type:none}ul.lst-kix_ok3a0zuyki56-1{list-style-type:none}.lst-kix_wdqip9honobb-4>li:before{content:"\0025cb   "}.lst-kix_wdqip9honobb-6>li:before{content:"\0025cf   "}.lst-kix_sdnmvxmx7mzu-7>li{counter-increment:lst-ctn-kix_sdnmvxmx7mzu-7}.lst-kix_wdqip9honobb-5>li:before{content:"\0025a0   "}ul.lst-kix_sbjhxah0gf4r-1{list-style-type:none}ul.lst-kix_sbjhxah0gf4r-0{list-style-type:none}.lst-kix_sdnmvxmx7mzu-0>li{counter-increment:lst-ctn-kix_sdnmvxmx7mzu-0}ul.lst-kix_sbjhxah0gf4r-5{list-style-type:none}ul.lst-kix_sbjhxah0gf4r-4{list-style-type:none}ol.lst-kix_sdnmvxmx7mzu-1.start{counter-reset:lst-ctn-kix_sdnmvxmx7mzu-1 0}ul.lst-kix_sbjhxah0gf4r-3{list-style-type:none}ul.lst-kix_sbjhxah0gf4r-2{list-style-type:none}.lst-kix_fwvmi50r8yw-5>li:before{content:"-  "}.lst-kix_fwvmi50r8yw-3>li:before{content:"-  "}.lst-kix_sbjhxah0gf4r-6>li:before{content:"-  "}.lst-kix_sbjhxah0gf4r-8>li:before{content:"-  "}.lst-kix_fwvmi50r8yw-7>li:before{content:"-  "}.lst-kix_nshgg3uvyo9d-4>li:before{content:"-  "}.lst-kix_2bdje4yk4iw-6>li:before{content:"-  "}.lst-kix_2bdje4yk4iw-8>li:before{content:"-  "}.lst-kix_nshgg3uvyo9d-2>li:before{content:"-  "}.lst-kix_nshgg3uvyo9d-6>li:before{content:"-  "}.lst-kix_2bdje4yk4iw-4>li:before{content:"-  "}.lst-kix_x1gdfe6t2wts-3>li:before{content:"-  "}.lst-kix_2bdje4yk4iw-0>li:before{content:"-  "}.lst-kix_nshgg3uvyo9d-8>li:before{content:"-  "}.lst-kix_2bdje4yk4iw-2>li:before{content:"-  "}.lst-kix_x1gdfe6t2wts-1>li:before{content:"-  "}.lst-kix_k7awirwczdtb-2>li:before{content:"-  "}.lst-kix_k7awirwczdtb-4>li:before{content:"-  "}.lst-kix_panwxdneeeuv-7>li:before{content:"-  "}.lst-kix_k7awirwczdtb-8>li:before{content:"-  "}.lst-kix_ok3a0zuyki56-0>li:before{content:"-  "}.lst-kix_k7awirwczdtb-6>li:before{content:"-  "}.lst-kix_ok3a0zuyki56-6>li:before{content:"-  "}ul.lst-kix_k7awirwczdtb-0{list-style-type:none}ul.lst-kix_k7awirwczdtb-1{list-style-type:none}.lst-kix_sdnmvxmx7mzu-0>li:before{content:"" counter(lst-ctn-kix_sdnmvxmx7mzu-0,decimal) ". "}ol.lst-kix_sdnmvxmx7mzu-0.start{counter-reset:lst-ctn-kix_sdnmvxmx7mzu-0 0}.lst-kix_ok3a0zuyki56-2>li:before{content:"-  "}.lst-kix_ok3a0zuyki56-4>li:before{content:"-  "}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}ul.lst-kix_wdqip9honobb-5{list-style-type:none}.lst-kix_sbjhxah0gf4r-4>li:before{content:"-  "}.lst-kix_fwvmi50r8yw-1>li:before{content:"-  "}ul.lst-kix_wdqip9honobb-6{list-style-type:none}ul.lst-kix_wdqip9honobb-3{list-style-type:none}ul.lst-kix_wdqip9honobb-4{list-style-type:none}.lst-kix_sbjhxah0gf4r-2>li:before{content:"-  "}ul.lst-kix_wdqip9honobb-7{list-style-type:none}ul.lst-kix_wdqip9honobb-8{list-style-type:none}.lst-kix_sbjhxah0gf4r-0>li:before{content:"-  "}ul.lst-kix_k7awirwczdtb-6{list-style-type:none}ul.lst-kix_sbjhxah0gf4r-8{list-style-type:none}.lst-kix_k7awirwczdtb-0>li:before{content:"-  "}ul.lst-kix_k7awirwczdtb-7{list-style-type:none}ul.lst-kix_sbjhxah0gf4r-7{list-style-type:none}ul.lst-kix_k7awirwczdtb-8{list-style-type:none}ul.lst-kix_sbjhxah0gf4r-6{list-style-type:none}ul.lst-kix_wdqip9honobb-1{list-style-type:none}.lst-kix_ok3a0zuyki56-8>li:before{content:"-  "}ul.lst-kix_k7awirwczdtb-2{list-style-type:none}ul.lst-kix_wdqip9honobb-2{list-style-type:none}ul.lst-kix_k7awirwczdtb-3{list-style-type:none}ul.lst-kix_k7awirwczdtb-4{list-style-type:none}ul.lst-kix_wdqip9honobb-0{list-style-type:none}ul.lst-kix_k7awirwczdtb-5{list-style-type:none}ol{margin:0;padding:0}table td,table th{padding:0}.c8{background-color:#ffffff;color:#1f1f1f;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10.5pt;font-family:"Roboto";font-style:normal}.c16{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:20pt;font-family:"Arial";font-style:normal}.c18{-webkit-text-decoration-skip:none;color:#0b57d0;font-weight:400;text-decoration:underline;text-decoration-skip-ink:none;font-size:10.5pt;font-family:"Roboto"}.c0{padding-top:16pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c3{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c9{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c13{color:#434343;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:normal}.c20{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c5{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c11{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial"}.c19{color:#000000;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c6{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c15{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c14{color:inherit;text-decoration:inherit}.c17{padding:0;margin:0}.c10{margin-left:36pt;padding-left:0pt}.c2{height:11pt}.c12{font-weight:700}.c7{font-style:italic}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c15 doc-content"><h1 class="c20" id="h.32o3xxsh1kk0"><span>FlashAttention</span><span class="c16">&nbsp;on Tenstorrent&rsquo;s Wormhole Architecture</span></h1><p class="c1"><span class="c4">Author: Colman Glagovich</span></p><h2 class="c9" id="h.9o3cjvqoget"><span>Abstract</span></h2><p class="c1"><span>The FlashAttention kernels have made great progress in speeding up scaled dot product attention on GPUs. We apply this algorithm to the TT-Metal software stack, targeting Tenstorrent&rsquo;s Wormhole accelerator. This paper applies the algorithm of FlashAttention-2 &ndash; &nbsp;parallelizing Q across multiple processors with online softmax on chunks of KV &ndash; and some kernel-level optimizations from FlashAttention-3 &ndash; pipelined data movement and asynchronous execution &ndash; to create FlashAttention on Wormhole which achieves a </span><span class="c12">20x speedup</span><span class="c4">&nbsp;over our Wormhole baseline. This work also applies 1) causality-aware parallelization for Tenstorrent&rsquo;s compute architecture and 2) overlapped data movement and computation with Tenstorrent&rsquo;s asynchronous Tensix cores.</span></p><h2 class="c9" id="h.ojj60noc6rwr"><span>1 Introduction</span></h2><p class="c1"><span class="c4">The GPU programming model relies on automatic fusion through torch compile and manual fusion of graphs such as FlashAttention to achieve peak performance. Fusion of all sorts reduces the load on global memory by increasing the arithmetic complexity of a kernel. This is a direct consequence of the restricted GPU programming model &ndash; a kernel must use global memory to pass data to the next kernel.</span></p><p class="c1"><span><br></span><span>The TT-Metal programming model is more flexible because it allows the outputs of a kernel to be allocated in either global memory or L1 SRAM. </span><span>We call the latter storage type &ldquo;L1 sharding&rdquo;, since the tensor storage is distributed across a grid of cores</span><span>.</span><span>&nbsp;This enables a form of op fusion specific to our framework </span><span>-</span><span>&nbsp;by making all the inputs and outputs of a graph of ops use </span><span>sharded</span><span>&nbsp;storage, TT-Metal removes the need for</span><span>&nbsp;round-trips </span><span>to DRAM between op invocations. This is a major source of optimization in the TT-Metal stack, allowing a sequence of ops to effectively &ldquo;fuse&rdquo; their data movement (and therefore reduce the</span><span>&nbsp;</span><span>global memory bandwidth load, improving performance) without requiring the user to write a custom fused </span><span>op</span><span class="c4">.</span></p><p class="c1 c2"><span class="c4"></span></p><p class="c1"><span>While sharding removes the need to write custom fused ops in most cases for Tenstorrent hardware, </span><span>some algorithms such as FlashAttention provide large algorithmic improvements over the original implementation and should</span><span>&nbsp;be implemented as a new fused op</span><span>.</span><span class="c4">&nbsp;When it is necessary to implement a custom fused op, the TT-Metal programming paradigm enables developers to experiment with and implement optimized kernels.</span></p><p class="c1 c2"><span class="c4"></span></p><p class="c1"><span>This whitepaper describes the implementation of FlashAttention on the TT-Metal software stack, targeting our </span><span>Wormhole</span><span class="c4">&nbsp;accelerator. We take advantage of (1) optimized load scheduling for causal attention which we apply to our grid of parallel cores, (2) TT-Metal&rsquo;s asynchronous execution model, enabling pipelining of data movement and compute, and (3) Wormhole&rsquo;s tile-based compute engines.</span></p><h2 class="c9" id="h.jlw3fgt53dj8"><span>2 </span><span>Background</span></h2><h3 class="c0" id="h.81pnid2zcvz2"><span>2.1 </span><span>Algorithm</span></h3><p class="c1"><span>Our first implementation of scaled dot product attention was the naive matmul -&gt; softmax -&gt; matmul. We were able to use sharded memory to &ldquo;fuse&rdquo; these operations together and reduce data movement, though this method did not scale to larger sequence lengths due to limited L1 space. For larger sequences we had to spill our intermediates to DRAM, leading to decreased performance. By applying the FlashAttention-2 algorithm, we take advantage of Wormhole&rsquo;s </span><span>120MB of L1 memory</span><span class="c4">&nbsp;(on-chip SRAM) and demonstrate the process of writing SOTA kernels on the TT-Metal stack. We also applied some optimizations from FlashAttention-3, including asynchronous data movement and compute.</span></p><h3 class="c0" id="h.5540dbw7gord"><span>2.2 </span><span>Wormhole architecture</span></h3><p class="c1"><span>The Wormhole architecture consists of an 8x10 grid of Tensix cores, connected in a 2-D torus with a NoC. Connected to the NoC is 12 channels of </span><span>1GB GDDR</span><span class="c4">6, totaling 288 GB/s DRAM bandwidth. In addition, Wormhole has 16 ethernet cores connected to the NoC which provide 3200 Gbps of bidirectional bandwidth for scaleout. </span></p><p class="c1 c2"><span class="c4"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 320.85px; height: 245.27px;"><img alt="" src="images/image6.png" style="width: 320.85px; height: 245.27px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c7">Figure 1: Wormhole logical NoC layout</span></p><p class="c1"><span class="c4"><br>Each Tensix core contains 1.5 MB of L1 memory (SRAM) and 5 programmable risc-v cores. RISC0 and RISC1 are capable of issuing NoC transfers to move data from L1 &lt;-&gt; L1 and L1 &lt;-&gt; DRAM. RISC 2, 3, and 4 are responsible for issuing instruction streams to our tile-based matrix processing engine which implements matrix multiplication, element-wise unary and binary operations, and other operations on tensors in L1. &nbsp;These RISCs operate concurrently, enabling native pipelining and data movement/compute overlap within a Tensix.</span></p><p class="c1 c2"><span class="c4"></span></p><p class="c1"><span class="c4">We do all of our development and benchmarking in this paper on Tenstorrent&rsquo;s n150 system, a PCIe card with a single Wormhole chip operating at up to 160W. </span></p><h3 class="c0" id="h.mmzkbjt10sg"><span class="c13">2.3 TT-Metal Execution Model</span></h3><p class="c1"><span>TT-Metal</span><span>&nbsp;executes one </span><span class="c12">program</span><span>&nbsp;(an operation like matmul, softmax, etc.) on a 2D grid of Tensix cores at a time. A program is a collection of </span><span class="c12">kernels</span><span>&nbsp;written in C++ which target the RISCs of a Tensix. A program typically consists of a reader kernel, writer kernel, and compute kernel where the reader targets RISC0, the writer targets RISC1, and the compute kernel is compiled to execute on RISCs 2, 3, and 4. We parallelize a program by mapping it onto a 2D grid of Tensix cores, each of which executes its reader/writer/compute kernels </span><span>asynchronously</span><span class="c4">. Contrast this with the GPU programming model, where a user launches a grid of blocks of threads. Blocks are scheduled on parallel processors and each thread is responsible for read/compute/write unless complex specialization and synchronization is implemented.</span></p><p class="c1"><span>See documentation for more details. </span><span class="c6"><a class="c14" href="https://www.google.com/url?q=https://github.com/tenstorrent/tt-metal/blob/main/METALIUM_GUIDE.md&amp;sa=D&amp;source=editors&amp;ust=1725476268994681&amp;usg=AOvVaw3Dafr1g6eyg6n2PgD-lwA8">https://github.com/tenstorrent/tt-metal/blob/main/METALIUM_GUIDE.md</a></span><span>.</span></p><h2 class="c9" id="h.h28oevvjk23t"><span>3 Implementation </span><span>Details</span></h2><p class="c1"><span>Our full, open-source implementation of FlashAttention is available on our github. It is used in LLMs such as Llama3-70B and Mixtral-8x7B. </span><span class="c18"><a class="c14" href="https://www.google.com/url?q=https://github.com/tenstorrent/tt-metal/tree/main/ttnn/cpp/ttnn/operations/transformer/sdpa&amp;sa=D&amp;source=editors&amp;ust=1725476268995324&amp;usg=AOvVaw1eSPmjgWrfyaJcsOG8XJU9">https://github.com/tenstorrent/tt-metal/tree/main/ttnn/cpp/ttnn/operations/transformer/sdpa</a></span></p><h3 class="c0" id="h.u5zbsl585zy0"><span class="c13">3.1 Parallelization</span></h3><p class="c1"><span class="c4">This section describes our causality-aware parallelization scheme which we developed to schedule work on a fixed grid of compute cores.</span></p><p class="c1"><span>A</span><span class="c4">lgorithmic optimizations we added to FlashAttention-2 include</span></p><ul class="c17 lst-kix_ek96qq2xmhch-0 start"><li class="c1 c10 li-bullet-0"><span class="c4">Causality-aware load balancing</span></li><li class="c1 c10 li-bullet-0"><span>Sparse attention mask reading for causal-attention</span></li></ul><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 609.27px; height: 310.59px;"><img alt="" src="images/image7.png" style="width: 609.27px; height: 310.59px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c7">Figure 2: Example chunking scheme of Q, K, and V inputs for multi-query attention with 8 query </span><span class="c7">heads</span><span class="c7">.</span></p><p class="c1 c2"><span class="c4"></span></p><p class="c1"><span class="c4">Attention has an output of shape `batch, num_heads, seq_len, dhead`. We parallelize the output tensor on the first three dimensions such that each Tensix core in the grid is responsible for computing attention for some portion of the `batch`, `heads`, and `seq_len`. This ensures that each core has a balanced amount of work.</span></p><p class="c1 c2"><span class="c4"></span></p><p class="c1 c2"><span class="c4"></span></p><p class="c1"><span class="c19 c12">3.1.1 Q, K, and V chunking</span></p><p class="c1"><span>Parallelizing over batch and num_heads is trivial since these dimensions are independent. Parallelizing over the sequence length interplays with our chunking factors in Q and K. </span><span>FlashAttention has two hyperparameters to choose: q_chunk_size and k_chunk_size. These determine the length of Q and K chunks in the sequence length </span><span class="c4">dimension. V shares the same chunk size as K. These hyperparameters affect FlashAttention by setting the size of Q, K, and V chunks which are read in the outer and inner loop of FlashAttention. They should be swept for a given input size to find the optimal configuration.</span></p><p class="c1 c2"><span class="c4"></span></p><p class="c1"><span class="c4">Given the batch and head parallelization, sequence length parallelization is described as the number of Q chunks each core must process.</span></p><p class="c1 c2"><span class="c4"></span></p><p class="c1"><span class="c12">3.1.2 Causality and load balancing</span></p><p class="c1 c2"><span class="c19 c12"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 621.09px; height: 303.36px;"><img alt="" src="images/image4.png" style="width: 621.09px; height: 303.36px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c7">Figure 3: Left: </span><span class="c7">original parallelization strategy for causal attention. Right: load balancing for causal attention and sparse attention mask reading.</span></p><p class="c1 c2"><span class="c19 c12"></span></p><p class="c1"><span>With some q_chunk_size, the Q tensor will be split into Q chunks Q0, Q1, &hellip;, Qn-1</span><span>. K and V are similar</span><span>ly chunked according to k_chunk_size. If we have </span><span class="c12">p</span><span>&nbsp;cores and </span><span class="c12">n</span><span>&nbsp;Q chunks, we assign </span><span class="c12">n/p</span><span class="c4">&nbsp;Q chunks to each core. </span></p><p class="c1"><span>In causal attention, </span><span>Q0 only has to attend to the chunks of K and V which are at equal or lesser token indices. So </span><span class="c4">if q_chunk_size == k_chunk_size, Q0 only attends to K0 (and V0), but Q1 attends to K0, K1 (and V0, V1).</span></p><p class="c1"><span class="c4">If we assigned consecutive Q chunks to each core, some cores would have more work to do than others. Our load balancing scheme solves this issue by assigning each core a &ldquo;low&rdquo; Q and a &ldquo;high&rdquo; Q. Core 0 computes Q0 and Qn-1. Core 1 computes Q1 and Qn-2, etc. This method perfectly balances the work across cores. This gives us 1.6x speedup over the naive parallelization strategy with unbalanced work.</span></p><p class="c1 c2"><span class="c4"></span></p><p class="c1"><span class="c4">Further taking advantage of causality, we assume a causal attention mask and apply sparsity such that the mask is only applied for score matrices on the diagonal. This leads to speedup by reducing DRAM pressure.</span></p><h3 class="c0" id="h.3zttfsubgu04"><span>3.2 </span><span class="c13">Asynchronous Execution and Pipelining</span></h3><p class="c1"><span class="c4">This section describes how we take advantage of Wormhole architecture features and the TT-Metal programming paradigm. The main features we take advantage of include</span></p><ul class="c17 lst-kix_9l64gnuy3fl7-0 start"><li class="c1 c10 li-bullet-0"><span class="c4">Asynchronous, pipelined data movement and compute</span></li><li class="c1 c10 li-bullet-0"><span class="c4">Tile-based data movement and compute</span></li></ul><p class="c1 c2"><span class="c4"></span></p><p class="c1"><span class="c19 c12">3.2.1 Kernel Pseudocode</span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 573.95px; height: 275.08px;"><img alt="" src="images/image5.png" style="width: 573.95px; height: 275.08px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c7">Figure 4: </span><span class="c7">Pseudocode for overlapped data movement and computation kernels</span></p><p class="c1 c2"><span class="c4"></span></p><p class="c1"><span class="c4">Our FlashAttention kernel reads Q, K, and V from DRAM and writes the output to DRAM. On each core, the reader kernel will read a chunk of Q and then iterate over the chunks of K and V. Intermediate results (accumulated output and statistics) are stored in L1 until all KV chunks are processed, then the output is written to DRAM.</span></p><p class="c1 c2"><span class="c4"></span></p><p class="c1"><span class="c19 c12">3.2.2 Pipelining</span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 177.33px;"><img alt="" src="images/image2.png" style="width: 624.00px; height: 177.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c11 c7">Figure 5: Within a single Tensix, the pipeline diagram of asynchronous data movement and compute in the FlashAttention op. Q, K, and V chunks are double-buffered in L1. Dotted lines indicate synchronization points between the concurrent reader, writer, and compute kernels. The first synchronization point indicates the compute kernel waiting for the first Q and K chunks to arrive in L1. The second indicates that the reader kernel waits for compute to consume V_chunk0 before reading V_chunk2 into L1, allowing the double buffer to free a slot. The third synchronization point indicates the writer kernel waiting for compute to produce the first output chunk.</span></p><p class="c1 c2"><span class="c4"></span></p><p class="c1"><span class="c4">Like all kernels written in the TT-Metal stack, our FlashAttention kernel takes advantage of concurrent reader, writer, and compute kernels to overlap data movement with compute. The RISCs within a Tensix synchronize using circular buffers, which can be thought of as thread-safe producer/consumer queues. We enable double buffering by sizing the circular buffers for Q, K, and V such that they can store 2 chunks at a time. Double buffering enables pipelining as shown in the above figure, hiding data movement latency by overlapping it with compute.</span></p><p class="c1 c2"><span class="c4"></span></p><p class="c1"><span class="c19 c12">3.2.3 Tiling</span></p><p class="c1"><span>TT-Metal also provides support for tile-based data movement and compute. In TT-metal, a tile is a </span><span>32x32</span><span>&nbsp;matrix that is contiguous in memory. &ldquo;Tilized&rdquo; tensors are tensors that have the last two dimensions shuffled such that the</span><span>&nbsp;elements of a 32x32 tile </span><span class="c4">are contiguous in memory. Reader/writer kernels benefit from tilized tensors because tile reads/writes are implemented as large contiguous bursts over NoC. Large bursts are effective at achieving high DRAM, NoC, and ethernet bandwidth. The compute engine in a Tensix natively operates on 32x32 tiles, so tilized tensors are also a natural format for the compute engine.</span></p><p class="c1 c2"><span class="c4"></span></p><p class="c1"><span>With tilized tensors and concurrent reader/writer/compute kernels, the implementation of our pipelined FlashAttention op is simple and idiomatic to the TT-Metal paradigm.</span></p><h2 class="c9" id="h.g3q6f7514nte"><span>4 Performance Analysis</span></h2><p class="c1"><span class="c4">We followed standard practice and measured performance for head_dim: {64, 128, 256} with hidden_dim fixed to 2048, and sequence length varied from 512 to 16k with batch size set such that the total number of tokens is 16k. We show results for BF16 and BFP8 input/output data format. BFP8 (block FP8) is a Tenstorrent-specific datatype where 16 datums share one exponent, and each datum has 7 bits of mantissa. The BFP8 datum size is approximately half of the BF16 datum size. Its range of possible values is greater than other FP8 formats like E4M3 and E5M2.</span></p><p class="c1"><span>Our baseline measurement is a standard implementation of </span><span>softmax attention executed on Wormhole which writes intermediates to DRAM.</span><span>&nbsp;FLOPs for causal attention is calculated as</span></p><p class="c5"><img src="images/image1.png"></p><p class="c1 c2"><span class="c8"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 602.67px;"><img alt="" src="images/image3.png" style="width: 624.00px; height: 602.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c11 c7">Figure 6: Performance of scaled dot product attention as head dim and sequence length varies. Left plots show results for BF16 dataformats and right plots show results for BFP8 dataformats.</span></p><p class="c1 c2"><span class="c4"></span></p><p class="c1"><span>The FlashAttention op on W</span><span class="c4">ormhole is on average 20x faster than the baseline implementation. Across all input sizes tested, the speedup ranges from 9x to 44x. FlashAttention is significantly faster than baseline because (1) intermediates remain in L1 rather than spill to DRAM, and (2) FlashAttention does no unnecessary work in causal attention while the baseline solution computes the full scores matrix.</span></p><p class="c1 c2"><span class="c4"></span></p><p class="c1"><span>We see moderate speedup between BF16 and BFP8 dataformats. We expect improvement because BFP8 inputs will require half the memory bandwidth as BF16 inputs. However, our results do not show a 2x speedup; this indicates that the op is not fully bound by memory bandwidth, but enters a regime where it is bound by matmul or softmax computation.</span></p><h2 class="c9" id="h.rby8t7tr0wow"><span>5 </span><span>Future work</span></h2><p class="c1"><span class="c4">There are three additional compute kernel optimizations we will apply to FlashAttention. Destination (DST) register reuse, in which one operand of a binary operation is reused from the DST register, will reduce the overhead of data movement within a Tensix core&rsquo;s L1. Automatic DST accumulation will allow us to remove all &ldquo;add&rdquo; operations from FlashAttention by using Tensix&rsquo;s accumulation register. We will also apply another optimization from FlashAttention-3 &ndash; pipelining matmul and softmax on different compute units to hide the latency of softmax.</span></p><p class="c1 c2"><span class="c4"></span></p><p class="c1"><span class="c4">One Wormhole feature that this work did not take advantage of is multicasting. Tensix cores can issue multicasts over the NoC to efficiently (and asynchronously) transfer data to any other core. Multi-query attention is a good use case for this feature, since cores parallelized on Q heads need to read the same K and V heads. We could reduce total DRAM traffic by using multicasting to share K and V heads between groups of cores.</span></p><p class="c1 c2"><span class="c4"></span></p><p class="c1"><span class="c4">We also plan on developing the backward pass of FlashAttention to enable efficient transformer training on Wormhole hardware. We expect to apply the same optimizations in the forward pass to the backward pass.</span></p><p class="c1 c2"><span class="c4"></span></p><p class="c1"><span class="c4">The various papers on FlashAttention deserve much credit.</span></p><p class="c1"><span>See the FlashAttention3 paper by Tri Dao et.al. at </span><span class="c6"><a class="c14" href="https://www.google.com/url?q=https://arxiv.org/pdf/2407.08608&amp;sa=D&amp;source=editors&amp;ust=1725476269004942&amp;usg=AOvVaw01C0b3BM1d9rwsmavDFW6W">https://arxiv.org/pdf/2407.08608</a></span></p><p class="c1 c2"><span class="c4"></span></p></body></html>
